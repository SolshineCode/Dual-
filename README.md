# Tandem Series LLM Training - Ongoing Research
License: CC-BY-SA

Part of an ongoing independant research project, whereby two LLMs are trained to operate in tandem, one feeding into the other in series.

I am working on creating and testing a step-by-step approach to fine-tuning a large language model using two models working in series. For the first approach, "Single Dynamic", I have one static and one dynamic, whereby one feeds into the next resulting in the desired result as well as an intermediary in-between point. This may help stack processing abilities and allow a further window into interpretability. For the second approach, "Dual Dynamic", I've modified the implementation to allow alternating training between the two models. This creates an adversarial-like training dynamic where each model takes turns learning to work with the other.

**Single Dynamic**

The problem involves fine-tuning a large language model (LLM) using a novel approach with two models:

1. **Static Model (a)**: This model remains unchanged during the fine-tuning process.
2. **Dynamic Model (b)**: This model is fine-tuned to learn how to communicate effectively with the static model (a).

The goal is to train model (b) to generate responses that, when fed into model (a), produce the desired output. This creates an "in-between space" where model (b) learns to adapt its output to elicit the correct response from model (a).

**Step-by-Step Approach**

To fine-tune model (b) using this approach, follow these steps:

1. **Prepare the training data**: Collect a dataset of input prompts and corresponding desired outputs. This dataset will be used to fine-tune model (b).
2. **Initialize model (b)**: Start with a pre-trained language model (e.g., BERT, RoBERTa, or a smaller variant of GPT-2 or GPT-J) as model (b).
3. **Freeze model (a)**: Ensure that model (a) remains unchanged throughout the fine-tuning process.
4. **Define the fine-tuning objective**: The objective is to minimize the difference between the output of model (a) and the desired output in the training data, given the input prompt and the response generated by model (b).
5. **Fine-tuning loop**:
	* Feed the input prompt from the training data to model (b).
	* Generate a response using model (b).
	* Feed the response from model (b) to model (a).
	* Compare the output of model (a) with the desired output in the training data.
	* Calculate the loss between the two outputs.
	* Backpropagate the loss to update the weights of model (b).
6. **Repeat the fine-tuning loop**: Iterate through the training data, updating model (b) after each iteration.
7. **Evaluate and refine**: Periodically evaluate the performance of model (b) on a validation set. Refine the fine-tuning process as needed.

**Diagram**
```markdown
+---------------+
|  Input Prompt  |
+---------------+
       |
       |
       v
+---------------+
|  Model (b)    |
|  (Dynamic)     |
+---------------+
       |
       |
       v
+---------------+
|  Model (a)    |
|  (Static)      |
+---------------+
       |
       |
       v
+---------------+
|  Desired Output|
+---------------+
       |
       |
       v
+---------------+
|  Loss Calculation|
+---------------+
       |
       |
       v
+---------------+
|  Update Model (b)|
+---------------+
```

I realize that I may have oversimplified the fine-tuning objective. In practice, the objective function may need to be more complex to account for the nuances of the in-between space between models (a) and (b). Additionally, the choice of model (b) and its initialization may significantly impact the fine-tuning process. May require experimenting with different models and hyperparameters to find the optimal configuration long term.

**Dual Dynamic**

