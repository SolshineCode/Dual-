{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Environment and Dependencies\n",
    "Install required packages including torch, transformers, and unsloth. Setup GPU environment on Colab T4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install torch transformers unsloth\n",
    "\n",
    "# Check if GPU is available\n",
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using CPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Required Libraries and Code\n",
    "Import necessary libraries and copy over the TwoModelTrainer code from TSL-Training-Single-Dynamic.py."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Required Libraries and Code\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments\n",
    "from typing import List, Dict, Tuple\n",
    "import logging\n",
    "import wandb\n",
    "from trl import SFTTrainer\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class PromptResponseDataset(Dataset):\n",
    "    \"\"\"Dataset for prompt-response pairs\"\"\"\n",
    "    def __init__(self, data: List[Dict[str, str]], tokenizer, max_length: int = 512):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = data\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        item = self.data[idx]\n",
    "        \n",
    "        # Tokenize input prompt\n",
    "        prompt_encoding = self.tokenizer(\n",
    "            item['prompt'],\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        # Tokenize target response\n",
    "        target_encoding = self.tokenizer(\n",
    "            item['target'],\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'prompt_ids': prompt_encoding['input_ids'].squeeze(),\n",
    "            'prompt_mask': prompt_encoding['attention_mask'].squeeze(),\n",
    "            'target_ids': target_encoding['input_ids'].squeeze(),\n",
    "            'target_mask': target_encoding['attention_mask'].squeeze()\n",
    "        }\n",
    "\n",
    "class TwoModelTrainer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        static_model_name: str,\n",
    "        dynamic_model_name: str,\n",
    "        tokenizer_name: str,\n",
    "        device: str = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    ):\n",
    "        self.device = device\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "        \n",
    "        # Initialize models\n",
    "        logger.info(f\"Loading static model {static_model_name}\")\n",
    "        self.static_model = AutoModelForCausalLM.from_pretrained(static_model_name).to(device)\n",
    "        self.static_model.eval()  # Set to evaluation mode\n",
    "        \n",
    "        logger.info(f\"Loading dynamic model {dynamic_model_name}\")\n",
    "        self.dynamic_model = AutoModelForCausalLM.from_pretrained(dynamic_model_name).to(device)\n",
    "        \n",
    "        # Freeze the static model\n",
    "        for param in self.static_model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def compute_loss(\n",
    "        self,\n",
    "        dynamic_output: torch.Tensor,\n",
    "        static_output: torch.Tensor,\n",
    "        target_ids: torch.Tensor,\n",
    "        target_mask: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Compute the combined loss between dynamic model output -> static model output -> target\n",
    "        \"\"\"\n",
    "        # Loss between static model output and target\n",
    "        static_loss = nn.CrossEntropyLoss(ignore_index=self.tokenizer.pad_token_id)(\n",
    "            static_output.view(-1, static_output.size(-1)),\n",
    "            target_ids.view(-1)\n",
    "        )\n",
    "        \n",
    "        # Additional loss terms could be added here, such as:\n",
    "        # - KL divergence between dynamic and static outputs\n",
    "        # - Auxiliary objectives for the dynamic model\n",
    "        # - Regularization terms\n",
    "        \n",
    "        return static_loss\n",
    "\n",
    "    def train(\n",
    "        self,\n",
    "        train_dataset: Dataset,\n",
    "        val_dataset: Dataset,\n",
    "        batch_size: int = 8,\n",
    "        num_epochs: int = 3,\n",
    "        learning_rate: float = 5e-5,\n",
    "        warmup_steps: int = 100,\n",
    "        gradient_accumulation_steps: int = 1,\n",
    "        max_grad_norm: float = 1.0,\n",
    "        log_wandb: bool = True\n",
    "    ):\n",
    "        \"\"\"Train the dynamic model\"\"\"\n",
    "        if log_wandb:\n",
    "            wandb.init(project=\"two-model-finetuning\")\n",
    "        \n",
    "        train_dataloader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=True\n",
    "        )\n",
    "        val_dataloader = DataLoader(\n",
    "            val_dataset,\n",
    "            batch_size=batch_size\n",
    "        )\n",
    "        \n",
    "        # Initialize optimizer and scheduler\n",
    "        optimizer = optim.AdamW(\n",
    "            self.dynamic_model.parameters(),\n",
    "            lr=learning_rate\n",
    "        )\n",
    "        \n",
    "        scheduler = optim.lr_scheduler.OneCycleLR(\n",
    "            optimizer,\n",
    "            max_lr=learning_rate,\n",
    "            epochs=num_epochs,\n",
    "            steps_per_epoch=len(train_dataloader)\n",
    "        )\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            self.dynamic_model.train()\n",
    "            total_loss = 0\n",
    "            \n",
    "            for step, batch in enumerate(train_dataloader):\n",
    "                # Move batch to device\n",
    "                batch = {k: v.to(self.device) for k, v in batch.items()}\n",
    "                \n",
    "                # Generate output from dynamic model\n",
    "                dynamic_output = self.dynamic_model(\n",
    "                    input_ids=batch['prompt_ids'],\n",
    "                    attention_mask=batch['prompt_mask']\n",
    "                ).logits\n",
    "                \n",
    "                # Pass dynamic output through static model\n",
    "                with torch.no_grad():\n",
    "                    static_output = self.static_model(\n",
    "                        input_ids=torch.argmax(dynamic_output, dim=-1),\n",
    "                        attention_mask=batch['prompt_mask']\n",
    "                    ).logits\n",
    "                \n",
    "                # Compute loss\n",
    "                loss = self.compute_loss(\n",
    "                    dynamic_output,\n",
    "                    static_output,\n",
    "                    batch['target_ids'],\n",
    "                    batch['target_mask']\n",
    "                )\n",
    "                \n",
    "                # Scale loss for gradient accumulation\n",
    "                loss = loss / gradient_accumulation_steps\n",
    "                loss.backward()\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "                \n",
    "                # Gradient accumulation and optimization\n",
    "                if (step + 1) % gradient_accumulation_steps == 0:\n",
    "                    torch.nn.utils.clip_grad_norm_(\n",
    "                        self.dynamic_model.parameters(),\n",
    "                        max_grad_norm\n",
    "                    )\n",
    "                    optimizer.step()\n",
    "                    scheduler.step()\n",
    "                    optimizer.zero_grad()\n",
    "                \n",
    "                # Log metrics\n",
    "                if log_wandb and step % 100 == 0:\n",
    "                    wandb.log({\n",
    "                        \"loss\": loss.item(),\n",
    "                        \"learning_rate\": scheduler.get_last_lr()[0],\n",
    "                    })\n",
    "                \n",
    "            # Validation loop\n",
    "            val_loss = self.evaluate(val_dataloader)\n",
    "            \n",
    "            logger.info(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "            logger.info(f\"Average training loss: {total_loss / len(train_dataloader)}\")\n",
    "            logger.info(f\"Validation loss: {val_loss}\")\n",
    "            \n",
    "            if log_wandb:\n",
    "                wandb.log({\n",
    "                    \"epoch\": epoch,\n",
    "                    \"train_loss\": total_loss / len(train_dataloader),\n",
    "                    \"val_loss\": val_loss\n",
    "                })\n",
    "\n",
    "    def evaluate(self, dataloader: DataLoader) -> float:\n",
    "        \"\"\"Evaluate the model on the validation set\"\"\"\n",
    "        self.dynamic_model.eval()\n",
    "        total_loss = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in dataloader:\n",
    "                batch = {k: v.to(self.device) for k, v in batch.items()}\n",
    "                \n",
    "                # Dynamic model generates initial output\n",
    "                dynamic_output = self.dynamic_model(\n",
    "                    input_ids=batch['prompt_ids'],\n",
    "                    attention_mask=batch['prompt_mask']\n",
    "                ).logits\n",
    "                \n",
    "                # Dynamic output is passed through static model\n",
    "                static_output = self.static_model(\n",
    "                    input_ids=torch.argmax(dynamic_output, dim=-1),\n",
    "                    attention_mask=batch['prompt_mask']\n",
    "                ).logits\n",
    "                \n",
    "                loss = self.compute_loss(\n",
    "                    dynamic_output,\n",
    "                    static_output,\n",
    "                    batch['target_ids'],\n",
    "                    batch['target_mask']\n",
    "                )\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "        \n",
    "        return total_loss / len(dataloader)\n",
    "\n",
    "    def save_model(self, path: str):\n",
    "        \"\"\"Save the dynamic model\"\"\"\n",
    "        self.dynamic_model.save_pretrained(path)\n",
    "        self.tokenizer.save_pretrained(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize Models and Tokenizer\n",
    "Initialize the Llama 3.2 1B and 3B models using unsloth, configure tokenizer with proper chat template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Models and Tokenizer\n",
    "\n",
    "from unsloth import FastLanguageModel\n",
    "from unsloth.chat_templates import get_chat_template\n",
    "\n",
    "# Initialize tokenizer with chat template\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"unsloth/Llama-3.2-1B-Instruct\")\n",
    "tokenizer = get_chat_template(tokenizer, chat_template=\"llama-3.1\")\n",
    "\n",
    "# Initialize models\n",
    "static_model_name = \"unsloth/Llama-3.2-1B-Instruct\"\n",
    "dynamic_model_name = \"unsloth/Llama-3.2-3B-Instruct\"\n",
    "\n",
    "logger.info(f\"Loading static model {static_model_name}\")\n",
    "static_model, _ = FastLanguageModel.from_pretrained(\n",
    "    model_name=static_model_name,\n",
    "    max_seq_length=2048,\n",
    "    dtype=None,\n",
    "    load_in_4bit=True\n",
    ")\n",
    "static_model = static_model.to(device)\n",
    "static_model.eval()  # Set to evaluation mode\n",
    "\n",
    "logger.info(f\"Loading dynamic model {dynamic_model_name}\")\n",
    "dynamic_model, _ = FastLanguageModel.from_pretrained(\n",
    "    model_name=dynamic_model_name,\n",
    "    max_seq_length=2048, \n",
    "    dtype=None,\n",
    "    load_in_4bit=True\n",
    ")\n",
    "dynamic_model = dynamic_model.to(device)\n",
    "\n",
    "# Freeze the static model\n",
    "for param in static_model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Dataset and Dataloaders\n",
    "Create PromptResponseDataset class and dataloaders for training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dataset and Dataloaders\n",
    "\n",
    "# Load and prepare FineTome dataset\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"mlabonne/FineTome-100k\", split=\"train\")\n",
    "\n",
    "# Convert ShareGPT format to HF format \n",
    "from unsloth.chat_templates import standardize_sharegpt\n",
    "dataset = standardize_sharegpt(dataset)\n",
    "\n",
    "# Apply proper chat template formatting\n",
    "def formatting_prompts_func(examples):\n",
    "    convos = examples[\"conversations\"]\n",
    "    texts = [tokenizer.apply_chat_template(convo, tokenize=False, add_generation_prompt=False) \n",
    "             for convo in convos]\n",
    "    return {\"text\": texts}\n",
    "\n",
    "dataset = dataset.map(formatting_prompts_func, batched=True)\n",
    "\n",
    "# Split into train/val\n",
    "train_size = int(0.9 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(\n",
    "    dataset, [train_size, val_size]\n",
    ")\n",
    "\n",
    "# Create dataloaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configure Two-Model Training\n",
    "Setup TwoModelTrainer with static and dynamic models, configure training parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure Two-Model Training\n",
    "\n",
    "# Initialize TwoModelTrainer\n",
    "trainer = TwoModelTrainer(\n",
    "    static_model_name=\"unsloth/Llama-3.2-1B-Instruct\",\n",
    "    dynamic_model_name=\"unsloth/Llama-3.2-3B-Instruct\",\n",
    "    tokenizer_name=\"unsloth/Llama-3.2-1B-Instruct\",\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# Configure training parameters\n",
    "batch_size = 8\n",
    "num_epochs = 3\n",
    "learning_rate = 5e-5\n",
    "warmup_steps = 100\n",
    "gradient_accumulation_steps = 1\n",
    "max_grad_norm = 1.0\n",
    "log_wandb = True\n",
    "\n",
    "# Train the model\n",
    "trainer.train(\n",
    "    train_dataset=train_dataset,\n",
    "    val_dataset=val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    num_epochs=num_epochs,\n",
    "    learning_rate=learning_rate,\n",
    "    warmup_steps=warmup_steps,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    max_grad_norm=max_grad_norm,\n",
    "    log_wandb=log_wandb\n",
    ")\n",
    "\n",
    "# Save the trained model\n",
    "trainer.save_model(\"path/to/save/model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Loop\n",
    "Run the training loop with gradient accumulation and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Loop\n",
    "\n",
    "# Initialize optimizer and scheduler\n",
    "optimizer = optim.AdamW(\n",
    "    trainer.dynamic_model.parameters(),\n",
    "    lr=learning_rate\n",
    ")\n",
    "\n",
    "scheduler = optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer,\n",
    "    max_lr=learning_rate,\n",
    "    epochs=num_epochs,\n",
    "    steps_per_epoch=len(train_dataloader)\n",
    ")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    trainer.dynamic_model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        # Move batch to device\n",
    "        batch = {k: v.to(trainer.device) for k, v in batch.items()}\n",
    "\n",
    "        # Generate output from dynamic model\n",
    "        dynamic_output = trainer.dynamic_model(\n",
    "            input_ids=batch['prompt_ids'],\n",
    "            attention_mask=batch['prompt_mask']\n",
    "        ).logits\n",
    "\n",
    "        # Pass dynamic output through static model\n",
    "        with torch.no_grad():\n",
    "            static_output = trainer.static_model(\n",
    "                input_ids=torch.argmax(dynamic_output, dim=-1),\n",
    "                attention_mask=batch['prompt_mask']\n",
    "            ).logits\n",
    "\n",
    "        # Compute loss\n",
    "        loss = trainer.compute_loss(\n",
    "            dynamic_output,\n",
    "            static_output,\n",
    "            batch['target_ids'],\n",
    "            batch['target_mask']\n",
    "        )\n",
    "\n",
    "        # Scale loss for gradient accumulation\n",
    "        loss = loss / gradient_accumulation_steps\n",
    "        loss.backward()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Gradient accumulation and optimization\n",
    "        if (step + 1) % gradient_accumulation_steps == 0:\n",
    "            torch.nn.utils.clip_grad_norm_(\n",
    "                trainer.dynamic_model.parameters(),\n",
    "                max_grad_norm\n",
    "            )\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        # Log metrics\n",
    "        if log_wandb and step % 100 == 0:\n",
    "            wandb.log({\n",
    "                \"loss\": loss.item(),\n",
    "                \"learning_rate\": scheduler.get_last_lr()[0],\n",
    "            })\n",
    "\n",
    "    # Validation loop\n",
    "    val_loss = trainer.evaluate(val_dataloader)\n",
    "\n",
    "    logger.info(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "    logger.info(f\"Average training loss: {total_loss / len(train_dataloader)}\")\n",
    "    logger.info(f\"Validation loss: {val_loss}\")\n",
    "\n",
    "    if log_wandb:\n",
    "        wandb.log({\n",
    "            \"epoch\": epoch,\n",
    "            \"train_loss\": total_loss / len(train_dataloader),\n",
    "            \"val_loss\": val_loss\n",
    "        })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Inference\n",
    "Test the trained model with example prompts (not using TextStreamer for output because it would need to be configured in-line between the models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Inference\n",
    "import torch\n",
    "\n",
    "# Enable fast inference optimizations\n",
    "FastLanguageModel.for_inference(trainer.dynamic_model)\n",
    "FastLanguageModel.for_inference(trainer.static_model)\n",
    "\n",
    "# Example prompts for testing\n",
    "example_prompts = [\n",
    "    {\"role\": \"user\", \"content\": \"What is the capital of France?\"}, \n",
    "    {\"role\": \"user\", \"content\": \"Explain the theory of relativity.\"},\n",
    "]\n",
    "\n",
    "# Tokenize inputs with chat template\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    example_prompts,\n",
    "    tokenize=True, \n",
    "    add_generation_prompt=True,\n",
    "    return_tensors=\"pt\"\n",
    ").to(device)\n",
    "\n",
    "# Generate with dynamic model\n",
    "dynamic_outputs = trainer.dynamic_model.generate(\n",
    "    input_ids=inputs[\"input_ids\"],\n",
    "    attention_mask=inputs[\"attention_mask\"], \n",
    "    max_new_tokens=64,\n",
    "    use_cache=True,\n",
    "    temperature=1.5,\n",
    "    min_p=0.1,\n",
    ")\n",
    "\n",
    "# Pass through static model\n",
    "with torch.no_grad():\n",
    "    static_outputs = trainer.static_model(\n",
    "        input_ids=dynamic_outputs,\n",
    "        attention_mask=torch.ones_like(dynamic_outputs).bool()\n",
    "    ).logits\n",
    "\n",
    "# Get final responses\n",
    "final_tokens = torch.argmax(static_outputs, dim=-1)\n",
    "responses = tokenizer.batch_decode(final_tokens, skip_special_tokens=True)\n",
    "\n",
    "# Display results\n",
    "for i, response in enumerate(responses):\n",
    "    print(f\"\\nPrompt {i+1}: {example_prompts[i]['content']}\")\n",
    "    print(f\"Response {i+1}: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save and Export Models\n",
    "Save the trained models in LoRA format and export to GGUF format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save and Export Models\n",
    "\n",
    "# Save the trained model in LoRA format\n",
    "trainer.dynamic_model.save_pretrained(\"TSL_lora_model\")\n",
    "tokenizer.save_pretrained(\"TSL_lora_model\")\n",
    "\n",
    "# Save the model in GGUF format\n",
    "if True:\n",
    "    trainer.dynamic_model.save_pretrained_gguf(\"TSL_model_gguf\", tokenizer)\n",
    "    # Remember to go to https://huggingface.co/settings/tokens for a token!\n",
    "    # And change hf to your username!\n",
    "    trainer.dynamic_model.push_to_hub_gguf(\"Solshine/TSL_model_gguf\", tokenizer, token=HF_TOKEN)\n",
    "\n",
    "# Save the model in 16bit GGUF format\n",
    "if True:\n",
    "    trainer.dynamic_model.save_pretrained_gguf(\"TSL_model_gguf_16bit\", tokenizer, quantization_method=\"f16\")\n",
    "    trainer.dynamic_model.push_to_hub_gguf(\"Solshine/TSL_model_gguf_16bit\", tokenizer, quantization_method=\"f16\", token=HF_TOKEN)\n",
    "\n",
    "# Save the model in q4_k_m GGUF format\n",
    "if False:\n",
    "    trainer.dynamic_model.save_pretrained_gguf(\"model_gguf_q4_k_m\", tokenizer, quantization_method=\"q4_k_m\")\n",
    "    trainer.dynamic_model.push_to_hub_gguf(\"hf/model_gguf_q4_k_m\", tokenizer, quantization_method=\"q4_k_m\", token=\"\")\n",
    "\n",
    "# Save the model in multiple GGUF formats\n",
    "if False:\n",
    "    trainer.dynamic_model.push_to_hub_gguf(\n",
    "        \"hf/model_gguf_multiple\",  # Change hf to your username!\n",
    "        tokenizer,\n",
    "        quantization_method=[\"q4_k_m\", \"q8_0\", \"q5_k_m\"],\n",
    "        token=\"\",  # Get a token at https://huggingface.co/settings/tokens\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
